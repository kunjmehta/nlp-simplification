{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataScience Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-cFxmTpZOa1",
        "colab_type": "code",
        "outputId": "05a7857d-3e01-4be3-8163-c4bc3efbf025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gf007nacG4b",
        "colab_type": "text"
      },
      "source": [
        "## GETTING DATA INTO COLAB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MR4_mNkZ0MV",
        "colab_type": "code",
        "outputId": "c2815fbe-5fea-445e-aa82-1d3fdf8ac405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "path = \"/content/drive/My Drive/submisisons/Final year/dataset/data/\"\n",
        "training_sentences = open(path + \"train.en\" , encoding = \"utf-8\").read().split(\"\\n\")\n",
        "tandrget_sentences = open(path + \"train.sen\" , encoding = \"utf-8\").read().split(\"\\n\")\n",
        "\n",
        "dev_complicated_sentences = open(path + \"dev.en\" , encoding = \"utf-8\").read().split(\"\\n\")\n",
        "dev_simple_sentences = open(path + \"dev.sen\" , encoding = \"utf-8\").read().split(\"\\n\")\n",
        "\n",
        "\n",
        "# can't print whole list because IO limits are being exceeded\n",
        "# try putting to file to see output\n",
        "print(training_sentences[1])\n",
        "print(target_sentences[1])\n",
        "\n",
        "print(dev_complicated_sentences[1])\n",
        "print(dev_simple_sentences[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gingerbread was brought to Europe in 992 by the Armenian monk Gregory of Nicopolis ( Gregory Makar ) ( Grégoire de Nicopolis ) .\n",
            "Armenian monk Gregory of Nicopolis ( Gregory Makar ) ( Grégoire de Nicopolis ) brought ginger bread to Europe in 992 .\n",
            "Adjacent counties are Marin ( to the south ) , Mendocino ( to the north ) , Lake ( northeast ) , Napa ( to the east ) , and Solano and Contra Costa ( to the southeast ) .\n",
            "Neighbhouring countries are Marin,to the south,  Mendocino, to the north, Lake northeast, Napa to the east and Solano and Contra Costa to the southeast.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMr_0y4ofmUs",
        "colab_type": "text"
      },
      "source": [
        "## TOKENIZING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IikJ7MNLbW3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "\n",
        "complicated = spacy.load('en')\n",
        "simple = spacy.load('en')\n",
        "\n",
        "def tokenize_complicated(sentence):\n",
        "    return [tok.text for tok in complicated.tokenizer(sentence)]\n",
        "\n",
        "def tokenize_simple(sentence):\n",
        "    return [tok.text for tok in simple.tokenizer(sentence)]\n",
        "\n",
        "C_TEXT = Field(tokenize=tokenize_complicated, fix_length = 100)\n",
        "C_TEXT_DEV = Field(tokenize=tokenize_complicated, fix_length = 100)\n",
        "\n",
        "# Can specify tensor type using \"d_type\". Default is long\n",
        "# \"fix_length\" : length of padding. Default is none\n",
        "# \"pad_first\": Default is false\n",
        "# \"batch_first\":  Whether to produce tensors with the batch dimension first. Default: False.\n",
        "S_TEXT = Field(tokenize=tokenize_simple, fix_length = 100, init_token = \"<sos>\", eos_token = \"<eos>\")\n",
        "S_TEXT_DEV = Field(tokenize=tokenize_simple, fix_length = 100, init_token = \"<sos>\", eos_token = \"<eos>\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuywynTUhfDR",
        "colab_type": "text"
      },
      "source": [
        "## MAKING INTO TABULAR FORMAT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "896CW0sWflFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STILL NEED TO ADD TEST DATA AS PER TUTORIAL. DON'T KNOW WHERE ITS NEEDED\n",
        "\n",
        "import pandas as pd\n",
        "raw_data = {'Complicated' : [line for line in training_sentences], \n",
        "            'Simple': [line for line in target_sentences]}\n",
        "\n",
        "raw_data_dev = {'Complicated' : [line for line in dev_complicated_sentences], \n",
        "            'Simple': [line for line in dev_simple_sentences]}\n",
        "\n",
        "df_train = pd.DataFrame(raw_data, columns=[\"Complicated\", \"Simple\"])\n",
        "df_dev = pd.DataFrame(raw_data_dev, columns=[\"Complicated\", \"Simple\"])\n",
        "\n",
        "df_train.to_csv(\"df_train.csv\", index=False)\n",
        "df_dev.to_csv(\"df_dev.csv\", index=False)\n",
        "\n",
        "\n",
        "# remove very long sentences and sentences where translations are \n",
        "# not of roughly equal length\n",
        "\n",
        "# DO WE NEED BELOW THING?\n",
        "\n",
        "# df['eng_len'] = df['English'].str.count(' ')\n",
        "# df['fr_len'] = df['French'].str.count(' ')\n",
        "# df = df.query('fr_len < 80 & eng_len < 80')\n",
        "# df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')\n",
        "\n",
        "# associate the text in the 'English' column with the EN_TEXT field, # and 'French' with FR_TEXT\n",
        "data_fields = [('Complicated', C_TEXT), ('Simple', S_TEXT)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pighM4wDosb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INDEXED TO 0 TO PREVENT ERROR. TAKE CARE IN CASE OF FUTURE ERRORS.\n",
        "# HAVE SENT YOU HANGOUTS LINK FOR THE RELATED ERROR WITHOUT INDEXING\n",
        "train, val = torchtext.data.TabularDataset.splits(path='./', train = \"df_train.csv\", validation = \"df_dev.csv\", format='csv', fields=data_fields, skip_header = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a98rR6UTbODU",
        "colab_type": "code",
        "outputId": "a1d3aaca-263e-4251-fdf3-97f8ae80c1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(type(train))\n",
        "print(len(train))\n",
        "# print(len(train[0]))\n",
        "print(type(train[0]))\n",
        "print (train[0].__dict__.keys())\n",
        "print(train[283421].Simple[:])\n",
        "\n",
        "print(type(val))\n",
        "print(len(val))\n",
        "# print(len(train[0]))\n",
        "print(type(val[0]))\n",
        "print (val[0].__dict__.keys())\n",
        "print(train[15000].Simple[:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torchtext.data.dataset.TabularDataset'>\n",
            "284678\n",
            "<class 'torchtext.data.example.Example'>\n",
            "dict_keys(['Complicated', 'Simple'])\n",
            "['The', 'part', 'of', 'the', 'potato', 'that', 'people', 'eat', 'is', 'a', 'tuber', 'that', 'grows', 'under', 'the', 'ground', '.']\n",
            "<class 'torchtext.data.dataset.TabularDataset'>\n",
            "16001\n",
            "<class 'torchtext.data.example.Example'>\n",
            "dict_keys(['Complicated', 'Simple'])\n",
            "['This', 'is', 'the', 'process', 'called', 'natural', 'selection', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQIBsc6yX83s",
        "colab_type": "text"
      },
      "source": [
        "## MAKING INTO TABULAR ALTERNATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouGXFHPsN1RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = torchtext.data.TabularDataset(path='./df.csv', format='csv', fields=data_fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqnUO8FaiQqK",
        "colab_type": "text"
      },
      "source": [
        "## BUILDING VOCAB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW1pu-1aiTe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C_TEXT.build_vocab(train,val)\n",
        "S_TEXT.build_vocab(train,val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSkIY1vsYjhP",
        "colab_type": "code",
        "outputId": "5de6a59b-9c8a-4727-a955-d832841b4ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(S_TEXT.vocab.stoi[\"hill\"])\n",
        "print(C_TEXT.vocab.stoi[\"hill\"])\n",
        "print(len(S_TEXT.vocab))\n",
        "print(len(C_TEXT.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3631\n",
            "5031\n",
            "151008\n",
            "184316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOkUl5JrZIJI",
        "colab_type": "text"
      },
      "source": [
        "## MAKING BATCH SIZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA0A1dLQoa3c",
        "colab_type": "code",
        "outputId": "0e94a6c2-1225-45b1-d2ea-cabc4c7a252b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_iter = BucketIterator(train, batch_size=20, \\\n",
        "sort_key=lambda x: len(x.Simple), shuffle=True)\n",
        "\n",
        "val_iter = BucketIterator(val, batch_size=20, \\\n",
        "sort_key=lambda x: len(x.Simple), shuffle=True)\n",
        "\n",
        "# 1 IS PADDING. HOW TO RESOLVE IT? DIDN'T UNDERSTAND TUTORIAL\n",
        "batch = next(iter(train_iter))\n",
        "batch_val = next(iter(val_iter))\n",
        "\n",
        "# print(batch.Simple)\n",
        "# print(batch.Complicated)\n",
        "print(len(batch.Simple))\n",
        "print(len(batch.Complicated))\n",
        "\n",
        "print(len(batch_val.Simple))\n",
        "print(len(batch_val.Complicated))\n",
        "\n",
        "# DIFFERENT LENGTHS WHY?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "100\n",
            "100\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHv_LMrFrUnP",
        "colab_type": "code",
        "outputId": "da81101f-2a86-4d5b-c1fc-09fd7a0f38f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(batch)\n",
        "print(batch_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 100]\n",
            "\t[.Complicated]:[torch.LongTensor of size 100x100]\n",
            "\t[.Simple]:[torch.LongTensor of size 100x100]\n",
            "\n",
            "[torchtext.data.batch.Batch of size 100]\n",
            "\t[.Complicated]:[torch.LongTensor of size 100x100]\n",
            "\t[.Simple]:[torch.LongTensor of size 100x100]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf9m2QABN7jL",
        "colab_type": "text"
      },
      "source": [
        "## MAKING BATCH SIZE ALTERNATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHhVRWB8OAJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_iter = BucketIterator.splits(train, batch_size = 20, sort_key=lambda x: len(x.Simple), \n",
        "#                                    sort_within_batch=False, shuffle = True)\n",
        "\n",
        "# batch = next(iter(train_iter))\n",
        "# # print(len(batch.Simple))\n",
        "# # print(len(batch.Complicated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQzF4uxVjIKL",
        "colab_type": "text"
      },
      "source": [
        "## TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfJqO1ZvpPBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 100):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] =  math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "                \n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        # print(x.shape)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        # print(seq_len)\n",
        "        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        return x\n",
        "\n",
        "\n",
        "input_pad = C_TEXT.vocab.stoi['<pad>']# creates mask with 0s wherever there is padding in the input\n",
        "def create_masks(input_seq, target_seq):\n",
        "  input_pad = C_TEXT.vocab.stoi['<pad>']# creates mask with 0s wherever there is padding in the input\n",
        "  input_msk = (input_seq != input_pad).unsqueeze(1)\n",
        "\n",
        "# create mask as before\n",
        "  target_pad = S_TEXT.vocab.stoi['<pad>']\n",
        "  target_msk = (target_seq != target_pad).unsqueeze(1)\n",
        "  size = target_seq.size(1) # get seq_len for matrix\n",
        "  \n",
        "  # BELOW STATEMENT ORIGINALLY HAD NP.ONES(1,SIZE,SIZE) BUT GOT ERROR\n",
        "  # CHANGED PARAMETERS AFTER SEEING THE FUNCTION CALL\n",
        "  # TAKE CARE IN CASE OF FUTURE ERRORS\n",
        "  nopeak_mask = np.triu(np.ones(shape = (1, size, size)),k=1).astype('uint8')\n",
        "  nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0)\n",
        "  target_msk = target_msk & nopeak_mask\n",
        "\n",
        "  return input_msk, target_msk\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # perform linear operation and split into h heads\n",
        "        \n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * h * sl * d_model\n",
        "       \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)# calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output\n",
        "\n",
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    \n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        \n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm\n",
        "\n",
        "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "    \n",
        "# build a decoder layer with two multi-head attention layers and\n",
        "# one feed-forward layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        \n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "        src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "    \n",
        "# We can then build a convenient cloning function that can generate multiple layers:\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        \n",
        "        # RANGE WAS N. CONVERTED TO N-1 TO AVOID ERROR. LOOKOUT FOR FUTURE ERRORS\n",
        "        for i in range(N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "      # we don't perform softmax on the output as this will be handled \n",
        "# automatically by our loss function\n",
        "\n",
        "\n",
        "# ----------------------HYPERPARAMS FOR MODEL------------------------------\n",
        "\n",
        "d_model = 512\n",
        "heads = 8\n",
        "N = 6\n",
        "src_vocab = len(C_TEXT.vocab)\n",
        "trg_vocab = len(S_TEXT.vocab)\n",
        "\n",
        "\n",
        "\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "        \n",
        "# this code is very important! It initialises the parameters with a\n",
        "# range of values that stops the signal fading or getting too big.\n",
        "# See this blog for a mathematical explanation\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "\n",
        "def train_model(epochs, print_every=1):\n",
        "    \n",
        "    # SEE POSITIONING OF BELOW 4 STATEMENTS WRT EACH EPOCH LOSS PRINTING FOR\n",
        "    # TRAINGING AND VALIDATION\n",
        "    \n",
        "#     model.train()\n",
        "    start = time.time()\n",
        "    temp = start\n",
        "#     total_loss = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        model.train()      \n",
        "#         start = time.time()\n",
        "#         temp = start\n",
        "        total_loss_train = 0\n",
        "        \n",
        "        for i, batch in enumerate(train_iter):            \n",
        "            src = batch.Complicated.transpose(0,1)\n",
        "            trg = batch.Simple.transpose(0,1)            \n",
        "            # the French sentence we input has all words except\n",
        "            # the last, as it is using each word to predict the next\n",
        "            \n",
        "            trg_input = trg[:, :-1]\n",
        "            \n",
        "            # the words we are trying to predict\n",
        "            \n",
        "            targets = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            # create function to make masks using mask code above\n",
        "            \n",
        "            src_mask, trg_mask = create_masks(src, trg_input)\n",
        "            \n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            optim.zero_grad()\n",
        "            \n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), targets)            \n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            \n",
        "            # LOSS.DATA WAS INDEXED [0] BUT BECAUSE ONLY 1 LINE TAKEN, INDEX REMOVED.\n",
        "            # MAY LEAD TO INCONSISTENCY. TAKE CARE\n",
        "            total_loss_train += loss.data\n",
        "            if (i + 1) % print_every == 0:\n",
        "                loss_avg_train = total_loss_train / print_every\n",
        "                print(\"TRAINING LOSS: time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % ((time.time() - start) // 60, epoch + 1, i + 1, loss_avg_train, time.time() - temp, print_every))\n",
        "                total_loss_train = 0\n",
        "                temp = time.time()\n",
        "                \n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "#         start = time.time()\n",
        "#         temp = start\n",
        "        total_loss_val = 0\n",
        "  \n",
        "        for i, batch in enumerate(val_iter):            \n",
        "            src = batch.Complicated.transpose(0,1)\n",
        "            trg = batch.Simple.transpose(0,1)            \n",
        "            # the French sentence we input has all words except\n",
        "            # the last, as it is using each word to predict the next\n",
        "            \n",
        "            trg_input = trg[:, :-1]\n",
        "            \n",
        "            # the words we are trying to predict\n",
        "            \n",
        "            targets = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            # create function to make masks using mask code above\n",
        "            \n",
        "            src_mask, trg_mask = create_masks(src, trg_input)\n",
        "            \n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            optim.zero_grad()\n",
        "            \n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), targets)            \n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            \n",
        "            # LOSS.DATA WAS INDEXED [0] BUT BECAUSE ONLY 1 LINE TAKEN, INDEX REMOVED.\n",
        "            # MAY LEAD TO INCONSISTENCY. TAKE CARE\n",
        "            total_loss_val += loss.data\n",
        "            if (i + 1) % print_every == 0:\n",
        "                loss_avg_val = total_loss_val / print_every\n",
        "                print(\"VALIDATION LOSS: time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % ((time.time() - start) // 60, epoch + 1, i + 1, loss_avg_val, time.time() - temp, print_every))\n",
        "                total_loss_val = 0\n",
        "                temp = time.time()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZL40zG-kn6E",
        "colab_type": "text"
      },
      "source": [
        "## TRAINING AND VALIDATING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULfqq0w6knAO",
        "colab_type": "code",
        "outputId": "a1c4357c-ef92-4963-a66e-49dbf585e0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_model(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING LOSS: time = 0m, epoch 1, iter = 1, loss = 11.950, 17s per 1 iters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_zrTup87rD5",
        "colab_type": "text"
      },
      "source": [
        "## INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4UIsbIy7tGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simplify(model, src, max_len = 100, custom_string=True):\n",
        "    \n",
        "    sentence = \"\"\n",
        "    model.eval()\n",
        "    \n",
        "    \n",
        "    # REQUIRES FILE PATH INPUT\n",
        "    # THEN TOKENISATION AND BATCH CREATION ALSO MAYBE. LOOKEY HERE:\n",
        "    #             src = batch.Complicated.transpose(0,1)\n",
        "    #             trg = batch.Simple.transpose(0,1)    \n",
        "    \n",
        "    \n",
        "    if custom_string == True:\n",
        "        src = tokenize_complicated(src)\n",
        "        sentence=\\\n",
        "        Variable(torch.LongTensor([[C_TEXT.vocab.stoi[tok] for tok\n",
        "        in sentence]])).cuda()\n",
        "#         print(src)\n",
        "        \n",
        "    src_mask = (src != input_pad).unsqueeze(-2)\n",
        "    e_outputs = model.encoder(src, src_mask)\n",
        "    \n",
        "    outputs = torch.zeros(max_len).type_as(src.data)\n",
        "    outputs[0] = torch.LongTensor([S_TEXT.vocab.stoi['<sos>']])\n",
        "    \n",
        "    for i in range(1, max_len):    \n",
        "            \n",
        "        # TAKE CARE HERE FOR NP.ONES PARAMETER LIST CHANGED.\n",
        "        # ORIGINALLY WAS TRG_MASK BOTH OF THEM. TRY INTERCHANGING IF PROBLEMS ARISE\n",
        "        src_mask = np.triu(np.ones(shape = (1, i, i),k=1)).astype('uint8')\n",
        "        trg_mask = Variable(torch.from_numpy(trg_mask) == 0).cuda()\n",
        "        \n",
        "        out = model.out(model.decoder(outputs[:i].unsqueeze(0), e_outputs, src_mask, trg_mask))\n",
        "        out = F.softmax(out, dim=-1)\n",
        "        val, ix = out[:, -1].data.topk(1)\n",
        "        \n",
        "        outputs[i] = ix[0][0]\n",
        "        if ix[0][0] == S_TEXT.vocab.stoi['<eos>']:\n",
        "            break\n",
        "                           \n",
        "    return ' '.join(\n",
        "    [S_TEXT.vocab.itos[ix] for ix in outputs[:i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g46_VIq9ql1",
        "colab_type": "text"
      },
      "source": [
        "## GIVING INPUT TO INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRyJ-LJ593Dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simplify(model, path + \"test.en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRvEH0MqimBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "print(sys.maxsize)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}